# openrouter.py

import os
import requests
import json
import re

OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1/chat/completions"
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")


def describe_architecture(name, components):
    """
        Generates a human-readable summary of an AWS architecture configuration.

        Args:
            name: The name or identifier of the architecture.
            components: A list of component dictionaries, each describing a service configuration.

        Returns:
            str: A formatted string describing the architecture's services, pricing models, regions,
                 compute resources, and storage types.
        """
    services = list({comp.get("service") for comp in components if comp.get("service")})
    pricing_models = list({comp.get("pricing_model") for comp in components if comp.get("pricing_model")})
    regions = {comp.get("region") for comp in components if comp.get("region")}
    compute_specs = [
        f"{comp['vcpu']} vCPUs, {comp.get('memory', '?')} GB RAM"
        for comp in components
        if comp.get("service") in ["EC2", "EKS"] and "vcpu" in comp
    ]
    storage_types = list({comp["service"] for comp in components if comp.get("service") in ["S3", "RDS"]})

    return f"""Architecture Name: {name}
Services: {', '.join(services)}
Pricing Models: {', '.join(pricing_models)}
Regions: {', '.join(regions) if regions else 'Not Specified'}
Compute Resources: {', '.join(compute_specs) if compute_specs else 'Not specified'}
Storage Types: {', '.join(storage_types) if storage_types else 'Not specified'}"""


SYSTEM_PROMPT = """
You are a cloud architecture analyst.

Evaluate the following AWS architecture for a text analytics platform that:
- Ingests and processes data in batches
- Runs hourly aggregation jobs
- Supports ad-hoc analytics and relational queries

The company operates in `il-central-1` and prioritizes:
1. Regional availability
2. Operational fit for batch + analytics + relational workloads
3. Future cost flexibility (serverless, spot, reserved pricing)

Rate the architecture (1â€“5) for each criterion and explain your reasoning.
"""


def query_openrouter(architecture_description, model="openai/gpt-3.5-turbo"):
    """
        Sends a request to the OpenRouter API with a given architecture description and returns the LLM's response.

        Args:
            architecture_description: A formatted string describing the architecture to be evaluated.
            model: The LLM model to use (default is 'openai/gpt-3.5-turbo').

        Returns:
            str: The response content generated by the LLM.

        Raises:
            Exception: If the API request fails or returns a non-200 status code.
    """
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": architecture_description}
        ]
    }

    response = requests.post(OPENROUTER_BASE_URL, headers=headers, json=payload)
    if response.status_code == 200:
        return response.json()["choices"][0]["message"]["content"]
    else:
        raise Exception(f"OpenRouter request failed: {response.status_code}, {response.text}")


def evaluate_all_architectures(architectures_path):
    """
        Loads a list of architecture configurations from a JSON file, evaluates each one using an LLM,
        and returns their scoring results.

        Args:
            architectures_path: Path to a JSON file containing architecture definitions.

        Returns:
            list: A list of dictionaries containing architecture names, evaluation scores for
                  Regional Availability, Operational Fit, Cost Flexibility, and Overall score.
                  If evaluation fails, the scores will be None and an error message will be included.
    """
    with open(architectures_path, "r") as f:
        data = json.load(f)

    results = []
    for architecture in data["architectures"]:
        name = architecture["metadata"]["architecture_id"]
        components = architecture.get("components", [])
        description = describe_architecture(name, components)
        print(f"Evaluating: {name}")
        try:
            response = query_openrouter(description)
            scores = extract_scores_from_llm_output(response)
            print(response)
            results.append({
                "Architecture": name,
                **scores
            })
        except Exception as e:
            print(f"Error evaluating {name}: {e}")
            results.append({
                "Architecture": name,
                "Regional Availability": None,
                "Operational Fit": None,
                "Cost Flexibility": None,
                "Overall": None,
                "Error": str(e)
            })

    return results


def extract_scores_from_llm_output(response_text):
    """
        Extracts numerical evaluation scores from the LLM-generated response text.

        Args:
            response_text (str): The raw response returned by the LLM.

        Returns:
            dict: A dictionary containing scores for:
                  - 'regional_availability'
                  - 'operational_fit'
                  - 'future_cost_flexibility'
                  - 'overall' (explicit or calculated average)
                  Each value is a number (int or float) or None if not found.
    """
    import re

    scores = {}
    criteria = {
        'regional_availability': r'Regional Availability.*?Rating: ?(\d)',
        'operational_fit': r'Operational Fit.*?Rating: ?(\d)',
        'future_cost_flexibility': r'Future Cost Flexibility.*?Rating: ?(\d)',
    }

    for key, pattern in criteria.items():
        match = re.search(pattern, response_text, re.IGNORECASE | re.DOTALL)
        if match:
            scores[key] = int(match.group(1))
        else:
            scores[key] = None

    # Try to extract an explicit overall score
    overall_match = re.search(r'Overall (?:Rating|Score)[:\s]+([0-5](?:\.\d+)?)', response_text, re.IGNORECASE)
    if overall_match:
        scores['overall'] = float(overall_match.group(1))
    else:
        # Fallback to average if all individual scores are available
        individual_scores = [v for v in scores.values() if isinstance(v, int)]
        if len(individual_scores) == 3:
            scores['overall'] = round(sum(individual_scores) / 3, 2)
        else:
            scores['overall'] = None

    return scores

